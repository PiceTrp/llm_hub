{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8109d4-1a18-49ec-966f-4e4507f925d9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a012ff-5ea0-4d59-bf6e-8221833d1551",
   "metadata": {},
   "source": [
    "- Guide: https://medium.com/towards-agi/how-to-use-vertex-ai-with-langchain-for-your-projects-ca7c1022a900\n",
    "- will do: https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d732dd30-33f2-4031-b88d-9c6c44bd1b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/picetrp/Documents/Learn/My_Self_Learn/llm_hub/get_started'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b3a4a0d-e4a0-408a-8a27-082d5d1fac08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup langsmith\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "#         prompt=\"Enter your OpenAI API key (required if using OpenAI): \"\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7abc1ac2-7f1d-43b5-bbde-ef4107f1b3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../serious-hold-453009-g1-eff08c861e11.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6bf164-bfef-458c-9bad-a048cccc41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your VertexAI credentials are configured\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "408c7f6e-51cb-4a7e-81c4-d7f9396b587d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatVertexAI(project='serious-hold-453009-g1', model_name='gemini-2.0-flash-001', full_model_name='projects/serious-hold-453009-g1/locations/us-central1/publishers/google/models/gemini-2.0-flash-001', client_options=ClientOptions: {'api_endpoint': 'us-central1-aiplatform.googleapis.com', 'client_cert_source': None, 'client_encrypted_cert_source': None, 'quota_project_id': None, 'credentials_file': None, 'scopes': None, 'api_key': None, 'api_audience': None, 'universe_domain': None}, default_metadata=(), model_family=<GoogleModelFamily.GEMINI_ADVANCED: '2'>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87322a9d-52d0-423a-a93e-12259f8c3021",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1ebc1-bab8-4a88-b776-32614e7c0a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "- learn from: https://learn.deeplearning.ai/courses/langchain/lesson/xf7wh/models,-prompts-and-parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acec556-f45b-4026-b4bf-b43b3fbd5e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f53bac-5028-4b2f-8271-6868a6080303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 13, 'candidates_token_count': 3, 'total_token_count': 16, 'prompt_tokens_details': [{'modality': 1, 'token_count': 13}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 3}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.004586287774145603, 'model_name': 'gemini-2.0-flash-001'}, id='run-e44a3661-7f03-4820-9673-09b357cf98b0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 3, 'total_tokens': 16})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate tfrom langchanihe following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e953e27-c0e4-4e35-9f94-3ba2d6424cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao|!\n",
      "|"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bbe93a-5705-44d0-aa54-6924496a655f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='Translate the following from English into {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Thai', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Thai', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "display(prompt_template)\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"Thai\", \"text\": \"hi!\"})\n",
    "\n",
    "display(prompt)\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f1b68e-9c41-4e89-9a85-d5d109c947b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¸ªà¸§à¸±à¸ªà¸”à¸µ! (Sawasdee!)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='à¸ªà¸§à¸±à¸ªà¸”à¸µ! (Sawasdee!)\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 9, 'candidates_token_count': 10, 'total_token_count': 19, 'prompt_tokens_details': [{'modality': 1, 'token_count': 9}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 10}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.15732163190841675, 'model_name': 'gemini-2.0-flash-001'}, id='run-e4be650b-e02a-403e-9fb6-53196e4e5d75-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b09068-20ec-4c6c-a862-6a0de832e4c1",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621644d4-9ada-44d7-850c-77a404de01bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"I went to the market and bought 201 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\\nLet's think step by step.\\n\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Start with 201 apples.\\n2. Give away 2 apples to the neighbor: 201 - 2 = 199 apples.\\n3. Give away 2 apples to the repairman: 199 - 2 = 197 apples.\\n4. Buy 5 more apples: 197 + 5 = 202 apples.\\n5. Eat 1 apple: 202 - 1 = 201 apples.\\n\\nSo, you remained with 201 apples.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 61, 'candidates_token_count': 121, 'total_token_count': 182, 'prompt_tokens_details': [{'modality': 1, 'token_count': 61}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 121}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.06128485734797706, 'model_name': 'gemini-2.0-flash-001'}, id='run-10fbaedd-3a45-463e-848c-fb26f519668e-0', usage_metadata={'input_tokens': 61, 'output_tokens': 121, 'total_tokens': 182})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# zero-shot COT\n",
    "template_string = \"\"\"I went to the market and bought {n_apples} apples. \\\n",
    "I gave 2 apples to the neighbor and 2 to the repairman. \\\n",
    "I then went and bought 5 more apples and ate 1. \\\n",
    "How many apples did I remain with?\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt = prompt_template.invoke({\"n_apples\": 201}) # => ChatPromptValue\n",
    "# prompt = prompt_template.format_messages(n_apples=7) # => List[HumanMessage]\n",
    "display(prompt)\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d416c5-a8ef-4c0f-8c9d-6c14c5353447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Start with 201 apples.\n",
      "2. Give away 2 apples to the neighbor: 201 - 2 = 199 apples.\n",
      "3. Give away 2 apples to the repairman: 199 - 2 = 197 apples.\n",
      "4. Buy 5 more apples: 197 + 5 = 202 apples.\n",
      "5. Eat 1 apple: 202 - 1 = 201 apples.\n",
      "\n",
      "So, you remained with 201 apples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c63aee-2a8e-4ba5-bc29-3fefd41f731e",
   "metadata": {},
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc546a8f-da32-46e1-95fc-0e99f76abe08",
   "metadata": {},
   "source": [
    "- used our openai key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05fbf4-8752-402d-a564-30df5c9d6f05",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eb1d128-7c5e-4ac9-a1f5-6fcf50ff617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# read local .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# initialize openai chat model - 2 ways\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# chat = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a9d5d7-ea5d-4b5a-bc57-e28e4112cc51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2_/7_kx30fj0tgb803zjjs198_40000gn/T/ipykernel_71384/2691146417.py:37: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\", \"I think it's worth it for the extra features\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2053d733-90d7-4df3-895b-c03273e8e5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3e2cb66-d20f-423c-bbc8-eb580fb635b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='gift', description='Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.', type='string')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bae3c6a-5961-49fa-a82d-a7c9ac3d759e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": \"True\",\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e820f96a-546b-4d21-b1f6-72593ff3a273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': 'True', 'delivery_days': '2', 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, dict)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "print(output_dict), type(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e9b12-9f1d-463e-9224-a0ffbeea7da0",
   "metadata": {},
   "source": [
    "### Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee2fd389-a01f-42af-96a8-d9b6f2f5864f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"sentiment\": string  // Was the comment shows a sign of positive or negative towards the subject? Answer True if yes, False if not or unknown if you are not sure.\\n\\t\"subject\": string  // What person do the comment mainly talking about? If this information is not found, output -1.\\n\\t\"reason\": string  // Extract any related sentences that is explained the sentiment of the comment. Output them as a comma separated Python list.\\n}\\n```'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'sentiment': 'Negative', 'subject': 'Snow White', 'reason': ['her hair is NOT A BOB', 'the original Snow White had what was called a French Roll', 'an incredibly difficult hair style that requires A LOT of hair', 'Most people canâ€™t even do a French Roll anymore without a wig', 'to keep a French roll in place? You have to Sew it into fabric', \"that 'bow' that snow has is actually what her hair is sewed to and wrapped around then tied at the top of her head\", 'thatâ€™s why her hair is so thick at the end', 'French Rolls take hours to do and typically would be kept in the hair for over a week']}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "comment = \"\"\"Okay for everyone wondering about Snow Whiteâ€™s hair hereâ€™s why it fails. \\\n",
    "Her hair is NOT A BOB the original Snow White had what was called a French Roll, an incredibly difficult hair style that requires A LOT of hair (you need at LEAST a foot of hair to pull it off and the more hair the easier itâ€™s to do). \\\n",
    "Most people canâ€™t even do a French Roll anymore without a wig because DRUMROLL PLEASE to keep a French roll in place? You have to Sew it into fabric. \\\n",
    "Yeah, that â€œbowâ€ that snow has is actually what her hair is sewed to and wrapped around then tied at the top of her head. \\\n",
    "Thatâ€™s why her hair is so thick at the end. French Rolls take hours to do and typically would be kept in the hair for over a week (no one wanted to undo it and redo it every day)\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "sentiment: Is the comment positive or negative towards the subject? Answer Positive if yes, Negative if no. Answer Unknown if you are not sure.\n",
    "subject: What person do the comment mainly talking about? If this information is not found, output -1.\n",
    "reason: Extract any related sentences that is explained the sentiment of the comment. Output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment: (str)\n",
    "subject: (str)\n",
    "reason: List[str]\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "sentiment = ResponseSchema(name=\"sentiment\", description=\"Was the comment shows a sign of positive or negative towards the subject? Answer True if yes, False if not or unknown if you are not sure.\")\n",
    "subject = ResponseSchema(name=\"subject\", description=\"What person do the comment mainly talking about? If this information is not found, output -1.\")\n",
    "reason = ResponseSchema(name=\"reason\", description=\"Extract any related sentences that is explained the sentiment of the comment. Output them as a comma separated Python list.\")\n",
    "\n",
    "response_schemas = [sentiment, subject, reason]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "display(format_instructions)\n",
    "print()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template)\n",
    "messages = prompt.format_messages(text=comment, \n",
    "                                format_instructions=format_instructions)\n",
    "response = chat(messages)\n",
    "response_dict = output_parser.parse(response.content)\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eab9773c-f276-4c48-b930-048063d350a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Positive', 'subject': 'She', 'reason': [\"She's the best part of the movie\", 'she really carried the movie']}\n"
     ]
    }
   ],
   "source": [
    "comment = \"\"\"I watched it yesterday. She's the best part of the movie, idc what anyone says but she really carried the movie.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template)\n",
    "messages = prompt.format_messages(text=comment, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "# try change temperature\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.0)\n",
    "response = chat(messages)\n",
    "response_dict = output_parser.parse(response.content)\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2031c3-ba57-4e4e-a8ab-deb503f39f0f",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Conversational Memory (langchain 0.2 - deprecated)\n",
    "- Memory classes (langchain 0.2 - deprecated) But still worth understanding: `**ConversationBufferMemory**`, `**ConversationBufferWindowMemory**`, `**ConversationTokenBufferMemory**`, `**ConversationSummaryMemory**`\n",
    "- Can read at: https://www.aurelio.ai/learn/langchain-conversational-memory or ask gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be15a11-93c0-4bc7-939b-f1457f359170",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "* **Core Idea:** This is the simplest form of memory. It stores the entire conversation history as a single string or a list of messages. Every interaction (user input and bot response) is appended to the buffer.\n",
    "\n",
    "* **Example:**\n",
    "    ```\n",
    "    User: Hello!\n",
    "    Bot: Hi there.\n",
    "    User: What's the capital of Thailand?\n",
    "    Bot: The capital of Thailand is Bangkok.\n",
    "    ```\n",
    "    **Memory Contents:** `\"Human: Hello!\\nAI: Hi there.\\nHuman: What's the capital of Thailand?\\nAI: The capital of Thailand is Bangkok.\"`\n",
    "\n",
    "## ConversationBufferWindowMemory\n",
    "\n",
    "* **Core Idea:** Keeps only the last `k` interactions (user input and bot response).\n",
    "\n",
    "* **Example (with `k=2`):**\n",
    "    ```\n",
    "    User: What's the weather like in Bangkok?\n",
    "    Bot: It's currently hot and humid.\n",
    "    User: And the temperature?\n",
    "    Bot: Around 32Â°C.\n",
    "    ```\n",
    "    **Memory Contents:** `\"Human: And the temperature?\\nAI: Around 32Â°C.\"` (The first two turns are discarded).\n",
    "\n",
    "## ConversationTokenBufferMemory\n",
    "\n",
    "* **Core Idea:** Stores recent interactions, limited by a maximum number of tokens.\n",
    "\n",
    "* **Example (with `max_token_limit=10`, simplified token count):**\n",
    "    ```\n",
    "    User: Hi (1 token)\n",
    "    Bot: Greetings (2 tokens)\n",
    "    User: Tell me something interesting about Bangkok. (7 tokens)\n",
    "    Bot: Bangkok is known for its vibrant street food. (7 tokens)\n",
    "    ```\n",
    "    **Memory Contents (might discard \"Hi\" and \"Greetings\" to stay within limit):** `\"Human: Tell me something interesting about Bangkok.\\nAI: Bangkok is known for its vibrant street food.\"`\n",
    "\n",
    "## ConversationSummaryMemory\n",
    "\n",
    "* **Core Idea:** Maintains a summarized version of the conversation over time.\n",
    "\n",
    "* **Example:**\n",
    "    ```\n",
    "    User: I'm planning a trip to Bangkok next month.\n",
    "    Bot: That sounds exciting!\n",
    "    User: What are some must-see attractions?\n",
    "    Bot: You should definitely visit the Grand Palace and Wat Arun.\n",
    "    ```\n",
    "    **Memory Contents (evolving summary):**\n",
    "    * **After first turn:** `\"The user is planning a trip to Bangkok.\"`\n",
    "    * **After second turn:** `\"The user is planning a trip to Bangkok and is asking for must-see attractions.\"`\n",
    "    * **After third turn:** `\"The user is planning a trip to Bangkok and has been recommended the Grand Palace and Wat Arun.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f0ba761-efbe-4484-9161-00223babd2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd7414-a81c-4e1b-8b0c-a4c6964a6352",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversational Memory (langchain 0.3+)\n",
    "- Can read at: https://www.aurelio.ai/learn/langchain-conversational-memory or ask gemini\n",
    "- We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a547d93-066e-40ae-af7e-1f1d0bd2d940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from getpass import getpass\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"] or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# # For normal accurate responses\n",
    "# llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6621d0a0-b46f-4ef2-aa63-b8f3e4fc77be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure your VertexAI credentials are configured\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a47bcd-9481-469a-8f16-df9d84792d0d",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory (Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe3c1fde-6689-4d1a-b53e-67c282fcb82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4dface5-7bca-4ffc-a9be-fdf73aa46a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is Pice\"},  # user message\n",
    "    {\"output\": \"Hey Pice, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f863150-ab11-4c07-9e74-f84dd81c0c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is Pice', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Pice, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63f12488-dd2b-4c4f-b107-95227486c368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is Pice', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Pice, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is Pice\")\n",
    "memory.chat_memory.add_ai_message(\"Hey Pice, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49c2f6fa-ddb6-4387-9cac-dba3814373b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2454b94b-f381-4c41-9194-a25a4a95f04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is Pice', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey Pice, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Pice.\\n', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is Pice', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Pice, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Pice.\\n', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I already told you, your name is Pice! I'm good at remembering things, so don't worry, I won't forget. ðŸ˜‰\\n\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"I already told you, your name is Pice! I'm good at remembering things, so don't worry, I won't forget. ðŸ˜‰\\n\"}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"what is my name again?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0698061c-f08b-4c09-8ac0-40f30a828359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I already told you, your name is Pice! I'm good at remembering things, so don't worry, I won't forget. ðŸ˜‰\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(response['response']) # cheeky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab48003-c69e-4e7a-ba11-6860e76802ac",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory (with RunnableWithMessageHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6690fee-c9d3-4169-81b5-6ebf36fdd407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'query'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x112be5e40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant called Begita.'), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Begita.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a5a637e-0a6d-4468-adb2-0ebc8a9006b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "018a4d38-b3de-498e-a632-8ef65bf56e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25e30dde-2e3d-4d8d-bf7a-a08e27edfaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e66b9b43-183c-4ee4-a7aa-824dc88e7e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Pice, it's nice to meet you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 16, 'candidates_token_count': 20, 'total_token_count': 36, 'prompt_tokens_details': [{'modality': 1, 'token_count': 16}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 20}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.060756373405456546, 'model_name': 'gemini-2.0-flash-001'}, id='run-812a6a27-b522-455c-9286-687e6c33253e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 20, 'total_tokens': 36})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Pice\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b2e4b39-b133-4596-a60d-018f641ad6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Pice.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 42, 'candidates_token_count': 7, 'total_token_count': 49, 'prompt_tokens_details': [{'modality': 1, 'token_count': 42}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 7}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.003015541338494846, 'model_name': 'gemini-2.0-flash-001'}, id='run-54264832-a470-46e5-9a14-cdb17d477d33-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "365c76c4-a637-494f-995c-4970151797f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As I recall, your name is Pice.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 55, 'candidates_token_count': 11, 'total_token_count': 66, 'prompt_tokens_details': [{'modality': 1, 'token_count': 55}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 11}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.12572927908463913, 'model_name': 'gemini-2.0-flash-001'}, id='run-656412be-f28d-47e0-add7-0d5cc187c5de-0', usage_metadata={'input_tokens': 55, 'output_tokens': 11, 'total_tokens': 66})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54bc8fca-d535-4300-9fe1-ece00bed44ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is Pice\n",
      "Hello Pice, it's nice to meet you! How can I help you today?\n",
      "\n",
      "What is my name again?\n",
      "Your name is Pice.\n",
      "\n",
      "What is my name again?\n",
      "As I recall, your name is Pice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing the chat history for session \"id_123\"\n",
    "history = get_chat_history(\"id_123\")\n",
    "\n",
    "# Print the entire history\n",
    "for message in history.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71672273-1a0e-470c-8312-bc44706c8906",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a71953-4c87-484a-8217-1455ddc4b205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f5834-df5b-4b77-9a7b-bad6613935c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553b356-8466-4bc0-a84d-b73d117ef9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93966a7-0ada-4f4b-bc85-5cb4e0b77d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ef61d-6fe9-4244-981a-c3b633411e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98401aed-6f21-4987-a25d-cdd4a0944be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "350cf148-50e0-4133-95df-4bee34a5e6a3",
   "metadata": {},
   "source": [
    "# Build a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d169f-d7c8-4a5a-afc9-a86f1818af6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c3ef0-2c1d-4c62-8f74-a1041eb34de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e15dcce-33af-40e4-bb87-33da9101a6b9",
   "metadata": {},
   "source": [
    "# Semantic Search in PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26f9c9-cb0d-4128-9188-8f52a0cf5e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hub",
   "language": "python",
   "name": "llm_hub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
